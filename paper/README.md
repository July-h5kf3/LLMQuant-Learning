<center>
    <h1>论文阅读笔记综述</h1>
</center>

这里是论文阅读的List，分为入门，进阶，前沿，综述三个板块,每篇文章阅读后，我会简单做一些小的介绍和笔记~

更加详细的论文阅读笔记以及思考见Note.md

### 入门

这部分论文无法选作文献汇报

- [ ] **AdaRound**:Up or Down? Adaptive Rounding for Post-Training Quantization (ICML 2020)

- [ ] **ZeroQuant**:Zeroquant: Efficient and affordable post-training quantization for large-scale transformers (NeurIPS 2022）

- [ ] **GPTQ**:Gptq: Accurate post-training quantization for generative pre-trained transformers (ICLR 2023)

- [ ] **AdaQuant**:Accurate post training quantization with small calibration sets (ICML 2021)

- [ ] **Smoothquant**:Accurate and efficient post-training quantization for large language models (ICML 2023)

- [ ] **SpinQuant**: Spinquant: Llm quantization with learned rotations (ICLR 2025)

- [ ] **Q-dit:Q-dit**: Accurate post-training quantization for diffusion transformers (CVPR 2025)
- [ ] **SVDQuant:Svdquant**: Absorbing outliers by low-rank components for 4-bit diffusion models (ICLR 2025)
- [ ] **Mpq-dm:Mpq-dm**: Mixed precision quantization for extremely low bit diffusion models (AAAI 2025)

### 进阶

这部分更多收集一些arXiv上比较好的工作以及一些会议的Spotlight和Oral以及Best Paper

### 前沿

这部分更多是收集一些大模型厂商的Technical Report，个人认为在资本趋利性下，对显存的”压榨“会做到极致

- [ ] **DeepSeekV3**：DeepSeek-V3 Technical Report

### 综述

这部分是一些综述，从综述入手一个领域是一个很Nice的选择，不一定要是paper可以是中文期刊甚至博客~

- [ ] A Survey on Model Compression for Large Language Model(TACL 2023)





Awesome系类：[pprp/Awesome-LLM-Quantization: Awesome list for LLM quantization](https://github.com/pprp/Awesome-LLM-Quantization)





