<center>
    <h1>大模型量化学习</h1>
</center>

本仓库为NKU2025年秋人工智能实践课，大模型量化选题个人仓库

### 课程总要求（课程评分 = 60平时作业+ 40期末答辩）

第一次专题汇报:专题论文自主汇报(PPT + 录屏  20%)

* 读懂论文，讲出重点
* 思路清晰
* 类白居易



第二次专题汇报:自选论文的pytorch复现报告(实验报告 20%)

- 复现完整，代码跑通
- 实验设计与分析的合理性解释
- 代码规范，带注释



第三次专题汇报:专题论文自主汇报(PPT + 录屏 20%)



同第一次专题汇报



课程大作业（整合前三次阶段性成果，实现高阶论文的Jittor实现与结果评估，完成综合展示与答辩 40%）

- 综合创新，融会贯通
- 创新表达，熟悉框架，讲清差异与思考
- 汇报有条理，自信从容



参考SparseTSF从torch框架到国产MindSpore框架的迁移：[July-h5kf3/SparseTSF-in-mindspore: SparseTSF在MindSpore上的代码迁移](https://github.com/July-h5kf3/SparseTSF-in-mindspore)



这是本人在2024年秋python高阶课实现的相关论文复现以及框架迁移



### 选题原因

一方面，我自大一以来便在密码学相关实验室实习，知识积累逐渐向自然语言处理（NLP）方向倾斜；另一方面，大模型已成为不可逆转的发展趋势，换句话说，生成式人工智能正引领未来。然而，制约大模型发展的关键瓶颈在于算力资源，尤其是显存。以消费级显卡为例，NVIDIA 于 2025 年推出了 RTX5090DD 显卡，以应对美国对华芯片出口限制。与已被阉割的 RTX5090D 相比，其最显著的削弱体现在显存资源上：从原版的 32GB GDDR7 减少到 24GB，位宽也从 512-bit 缩减至 384-bit。由此可见，在深度学习领域，显存堪称“黄金”。因此，如何高效利用显存，是大模型发展过程中无法回避的核心问题。我本人曾深入参与多项基于 GPU 及国产 GPGPU 芯片的优化竞赛，深刻体会到算法在提升显存利用效率方面所蕴含的巨大潜力。



DeepSeekV3 一经发布，便在学术界和工业界引发了广泛关注，其极低的训练成本更是令人震惊。其核心优势在于采用了 FP8 量化方案：通过创新的量化策略与高效的计算流程，不仅显著降低了模型的存储需求和计算资源占用，同时依然能够保持较高的精度。



因此，选题一经发布，看到大模型量化选题，我感觉既熟悉又兴奋，因此毫不犹豫地选择了这一选题，希望我能学有所获~

### 当前主要挑战

- 精度损失问题：量化过程通常会导致模型精度的下降，尤其是在低比特量化（如4位或更低）时，如何在压缩模型的同时保持性能是一个核心难题。
- 量化算法复杂性：现有的量化方法（如量化感知训练QAT、后训练量化PTQ）在实现上较为复杂，特别是在处理异构结构和动态范围较大的模型时，算法设计和调优难度高。
- 硬件适配度：不同硬件平台对量化模型的支持程度不一，缺乏统一的量化标准和硬件加速支持，导致量化模型的跨平台兼容性不足。
- 动态量化需求:在动态任务场景（如在线学习或多任务处理）中，模型需要实时调整量化策略，当前的静态量化方法难以满足这种需求。

### 学习资源

[HDU的课程资料Wiki](https://hdu-cs.wiki/3.AI%E6%A8%A1%E5%9D%97/3.10%E3%80%90LLM%E3%80%91%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/3.10.5LLM%E5%AE%9E%E6%93%8D%E9%A1%B9%E7%9B%AE/3.10.5.6Huggingface%E7%9A%84%E9%87%8F%E5%8C%96%E5%9F%BA%E7%A1%80.html)

[B站上一个Up](https://space.bilibili.com/18235884/lists/2887562?type=season)

[知乎一个专栏](https://zhuanlan.zhihu.com/p/557859725)

[知乎另一个专栏-奶龙](https://zhuanlan.zhihu.com/p/29140505773)
